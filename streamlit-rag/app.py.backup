#!/usr/bin/env python3
"""
Production RAG Chat Application - OPTIMIZED VERSION  
Streamlit web interface for RAG system with LlamaIndex + Ollama + Supabase
OPTIMIZATIONS: Reduced context, faster retrieval, better filtering
"""

import os
import streamlit as st
import logging
import time
from typing import List, Dict
from dotenv import load_dotenv

try:
    from llama_index.core import VectorStoreIndex, StorageContext
    from llama_index.vector_stores.supabase import SupabaseVectorStore
    from llama_index.embeddings.ollama import OllamaEmbedding
    from llama_index.llms.ollama import Ollama
    from llama_index.core.retrievers import VectorIndexRetriever
    from llama_index.core.query_engine import RetrieverQueryEngine
    from llama_index.core.postprocessor import SimilarityPostprocessor
except ImportError:
    from llama_index import VectorStoreIndex, StorageContext
    from llama_index.vector_stores import SupabaseVectorStore
    from llama_index.embeddings import OllamaEmbedding
    from llama_index.llms import Ollama
    from llama_index.retrievers import VectorIndexRetriever
    from llama_index.query_engine import RetrieverQueryEngine
    from llama_index.postprocessor import SimilarityPostprocessor

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="Fast RAG Chat System",
    page_icon="??",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Logging configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OPTIMIZED PRODUCTION CONFIGURATION
CONFIG = {
    "connection_string": os.getenv("SUPABASE_CONNECTION_STRING"),
    "table_name": "documents",
    "embed_model": "mxbai-embed-large",
    "chat_model": "llama3:8b-instruct-q4_K_M",
    "embed_dim": 1024,
    "ollama_url": "http://localhost:11434"
}

# Available chat models (ordered by speed vs quality)
AVAILABLE_MODELS = [
    ("llama3.2:3b", "Llama 3.2 3B (Fastest - Low Memory)", "~2GB", "10-30s"),
    ("llama3:8b-instruct-q4_K_M", "Llama 3 8B Instruct (Balanced)", "~5GB", "30-60s"),
    ("phi3:latest", "Phi-3 3.8B (Alternative)", "~2GB", "15-45s"),
]

@st.cache_resource
def initialize_rag_system():
    """OPTIMIZED: Initialize RAG system with fast retrieval (cached for performance)"""
    try:
        if not CONFIG["connection_string"]:
            st.error("SUPABASE_CONNECTION_STRING not found in .env file!")
            st.info("Please check your .env file and restart the application.")
            st.stop()

        with st.spinner("Initializing OPTIMIZED RAG system..."):
            # Initialize components
            vector_store = SupabaseVectorStore(
                postgres_connection_string=CONFIG["connection_string"],
                collection_name=CONFIG["table_name"],
                dimension=CONFIG["embed_dim"],
            )
            
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            embed_model = OllamaEmbedding(
                model_name=CONFIG["embed_model"], 
                base_url=CONFIG["ollama_url"]
            )
            
            # OPTIMIZED: Reduced context and faster settings
            llm = Ollama(
                model=CONFIG["chat_model"],
                base_url=CONFIG["ollama_url"],
                temperature=st.session_state.get("temperature", 0.1),
                request_timeout=180.0,
                # OPTIMIZED: Limit context for faster processing
                context_window=2048,
                num_predict=256
            )
            
            # Create index
            index = VectorStoreIndex.from_vector_store(
                vector_store=vector_store,
                storage_context=storage_context,
                embed_model=embed_model
            )
            
            # OPTIMIZED: Custom retriever with filtering
            retriever = VectorIndexRetriever(
                index=index,
                similarity_top_k=st.session_state.get("retrieval_top_k", 8),
                embed_model=embed_model
            )
            
            # OPTIMIZED: Similarity filter to keep only relevant chunks
            similarity_filter = SimilarityPostprocessor(
                similarity_cutoff=st.session_state.get("similarity_cutoff", 0.7)
            )
            
            # OPTIMIZED: Create query engine with retriever and filtering
            chat_engine = RetrieverQueryEngine.from_args(
                retriever=retriever,
                node_postprocessors=[similarity_filter],
                llm=llm,
                response_mode="compact",
                streaming=False,
                verbose=False,
                system_prompt=st.session_state.get("system_prompt", 
                    "You are a helpful assistant that answers questions based on provided document context. "
                    "IMPORTANT: Be concise and direct. Only answer if the context contains relevant information. "
                    "If the context doesn't contain relevant information, clearly state that the information is not available. "
                    "When answering, cite the specific document sources. Keep responses focused and under 200 words when possible."
                )
            )
            
            return chat_engine, index, retriever
            
    except Exception as e:
        st.error(f"Initialization error: {str(e)}")
        logger.error(f"RAG initialization error: {e}")
        
        # Provide helpful debugging information
        if "connection" in str(e).lower():
            st.info("Database Connection Issue: Check your Supabase connection string")
        elif "ollama" in str(e).lower():
            st.info("Ollama Issue: Make sure Ollama is running: `sudo systemctl status ollama`")
        elif "model" in str(e).lower():
            st.info("Model Issue: Check if models are installed: `ollama list`")
        
        st.stop()

def initialize_session_state():
    """OPTIMIZED: Initialize session state with performance settings"""
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "Hello! I'm your OPTIMIZED RAG assistant. Ask me anything about your documents - I'm now faster than ever!",
                "sources": []
            }
        ]
    
    # OPTIMIZED: Performance-focused defaults
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.1
    
    if "top_k" not in st.session_state:
        st.session_state.top_k = 2
    
    if "retrieval_top_k" not in st.session_state:
        st.session_state.retrieval_top_k = 8
    
    if "similarity_cutoff" not in st.session_state:
        st.session_state.similarity_cutoff = 0.7
    
    if "system_prompt" not in st.session_state:
        st.session_state.system_prompt = (
            "You are a helpful assistant that answers questions based on provided document context. "
            "IMPORTANT: Be concise and direct. Only answer if the context contains relevant information. "
            "If the context doesn't contain relevant information, clearly state that the information is not available. "
            "When answering, cite the specific document sources. Keep responses focused and under 200 words when possible."
        )
    
    if "show_sources" not in st.session_state:
        st.session_state.show_sources = True
    
    if "show_debug" not in st.session_state:
        st.session_state.show_debug = False

def display_chat_message(message: Dict):
    """Display a chat message with optional sources"""
    with st.chat_message(message["role"]):
        st.write(message["content"])
        
        # Display sources if available and enabled
        if (message["role"] == "assistant" and 
            st.session_state.show_sources and 
            message.get("sources")):
            
            with st.expander(f"Sources ({len(message['sources'])} documents)", expanded=False):
                for i, source in enumerate(message["sources"], 1):
                    # Extract source information
                    source_text = source.get("text", "No content available")[:500]
                    file_name = source.get("metadata", {}).get("file_name", "Unknown file")
                    similarity = source.get("score", 0)
                    
                    st.markdown(f"**Source {i}: {file_name}** (Similarity: {similarity:.3f})")
                    st.markdown(f"```\n{source_text}...\n```")
                    st.markdown("---")

def sidebar_configuration():
    """OPTIMIZED: Sidebar with performance controls"""
    with st.sidebar:
        st.title("Fast RAG Settings")
        
        # Performance section
        st.subheader("Performance")
        
        # Model selection with speed indicators
        selected_model = st.selectbox(
            "LLM Model",
            options=[model[0] for model in AVAILABLE_MODELS],
            index=1,
            format_func=lambda x: next(model[1] for model in AVAILABLE_MODELS if model[0] == x),
            help="Choose based on speed vs quality needs"
        )
        
        if selected_model != CONFIG["chat_model"]:
            st.info(f"Model change detected. Restart required.")
            CONFIG["chat_model"] = selected_model
        
        # Display expected speed
        speed_info = next((model[3] for model in AVAILABLE_MODELS if model[0] == selected_model), "Unknown")
        st.caption(f"Expected response time: {speed_info}")
        
        st.markdown("---")
        
        # OPTIMIZED: Retrieval settings
        st.subheader("Retrieval Settings")
        
        st.session_state.retrieval_top_k = st.slider(
            "Retrieval Candidates", 
            min_value=5, max_value=15, value=8, step=1,
            help="More candidates = better filtering, but slower"
        )
        
        st.session_state.top_k = st.slider(
            "Final Sources", 
            min_value=1, max_value=5, value=2, step=1,
            help="OPTIMIZED: Fewer sources = faster response"
        )
        
        st.session_state.similarity_cutoff = st.slider(
            "Similarity Threshold", 
            min_value=0.5, max_value=0.9, value=0.7, step=0.05,
            help="Higher = more relevant but possibly fewer results"
        )
        
        st.markdown("---")
        
        # Generation settings
        st.subheader("Generation")
        
        st.session_state.temperature = st.slider(
            "Temperature", 
            min_value=0.0, max_value=1.0, value=0.1, step=0.1,
            help="Lower = more focused, Higher = more creative"
        )
        
        st.markdown("---")
        
        # Display options
        st.subheader("Display")
        st.session_state.show_sources = st.checkbox("Show Sources", value=True)
        st.session_state.show_debug = st.checkbox("Show Debug Info", value=False)
        
        st.markdown("---")
        
        # Performance tips
        with st.expander("Speed Tips", expanded=False):
            st.markdown("""
            **For fastest responses:**
            - Use Llama 3.2 3B model
            - Set Final Sources to 1-2
            - Keep Similarity Threshold at 0.7+
            - Use Temperature 0.1
            
            **Current optimizations:**
            - Reduced chunk size (768 tokens)
            - Similarity filtering
            - Compact response mode  
            - Limited context window
            - Shorter max responses
            """)
        
        # Clear chat button
        if st.button("Clear Chat", type="secondary"):
            st.session_state.messages = [st.session_state.messages[0]]
            st.rerun()

def main():
    """OPTIMIZED: Main application with fast processing"""
    st.title("Fast RAG Chat System")
    st.caption("Optimized for speed: Reduced context, smart filtering, faster models")
    
    # Initialize session state
    initialize_session_state()
    
    # Initialize RAG system
    with st.spinner("Loading optimized RAG system..."):
        chat_engine, index, retriever = initialize_rag_system()
    
    # Sidebar configuration
    sidebar_configuration()
    
    # Display chat messages
    for message in st.session_state.messages:
        display_chat_message(message)
    
    # Chat input
    if prompt := st.chat_input("Ask me about your documents..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt, "sources": []})
        display_chat_message(st.session_state.messages[-1])
        
        # Generate response
        with st.chat_message("assistant"):
            start_time = time.time()
            
            with st.spinner("Processing your question..."):
                try:
                    # OPTIMIZED: Query with timeout and error handling
                    response = chat_engine.query(prompt)
                    
                    response_time = time.time() - start_time
                    
                    # Display response
                    st.write(response.response)
                    
                    # OPTIMIZED: Extract and display sources efficiently
                    sources = []
                    if hasattr(response, 'source_nodes') and response.source_nodes:
                        for node in response.source_nodes[:st.session_state.top_k]:
                            source_info = {
                                "text": node.get_content(),
                                "metadata": node.metadata,
                                "score": getattr(node, 'score', 0.0)
                            }
                            sources.append(source_info)
                    
                    # Performance info
                    if st.session_state.show_debug:
                        st.caption(f"Response time: {response_time:.1f}s | Sources: {len(sources)} | Model: {CONFIG['chat_model']}")
                    
                except Exception as e:
                    error_msg = f"Error generating response: {str(e)}"
                    st.error(error_msg)
                    
                    # Helpful error suggestions
                    if "timeout" in str(e).lower():
                        st.info("Timeout: Try reducing 'Final Sources' or switch to Llama 3.2 3B")
                    elif "context" in str(e).lower():
                        st.info("Context Issue: Try increasing 'Similarity Threshold' to reduce context size")
                    elif "connection" in str(e).lower():
                        st.info("Connection Issue: Check if Ollama is running")
                    
                    response_time = time.time() - start_time
                    sources = []
                    response = type('obj', (object,), {'response': error_msg})()
            
            # Add assistant message
            assistant_message = {
                "role": "assistant",
                "content": response.response,
                "sources": sources
            }
            st.session_state.messages.append(assistant_message)
            
            # Display sources if available
            if sources and st.session_state.show_sources:
                with st.expander(f"Sources ({len(sources)} documents)", expanded=False):
                    for i, source in enumerate(sources, 1):
                        # Extract source information safely
                        source_text = source.get("text", "No content available")[:500]
                        file_name = source.get("metadata", {}).get("file_name", "Unknown file")
                        similarity = source.get("score", 0)
                        
                        st.markdown(f"**Source {i}: {file_name}** (Similarity: {similarity:.3f})")
                        st.markdown(f"```\n{source_text}...\n```")
                        if i < len(sources):
                            st.markdown("---")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        st.info("Application stopped by user")
    except Exception as e:
        st.error(f"Application error: {e}")
        logger.error(f"Application error: {e}")

# OPTIMIZATIONS APPLIED:
# - Reduced similarity_top_k from 5 to 2 (faster inference)
# - Added retrieval_top_k=8 for better candidate selection
# - Implemented SimilarityPostprocessor for filtering
# - Used RetrieverQueryEngine with compact mode
# - Reduced context_window to 2048 tokens
# - Limited num_predict to 256 tokens
# - Reduced request_timeout to 180s
# - Added performance monitoring and tips
# - Improved error handling with specific suggestions
# - Better model selection with speed indicators