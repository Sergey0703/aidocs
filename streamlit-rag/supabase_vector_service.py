# ====================================
# ????: /opt/streamlit-rag/supabase_vector_service.py
# ????????? ?????? ?????? ????? Supabase ? ?????????????? LlamaIndex
# ====================================

"""
Supabase Vector Service - ????????? ????? ? ?????????????? ???? ??????
??????????? ?????? ? ????????????? ????????? ?? LlamaIndex
"""

import os
import time
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """????????? ?????????? ??????"""
    content: str
    full_content: str
    metadata: Dict[str, Any]
    similarity_score: float
    document_id: str
    filename: str
    chunk_index: int
    search_info: Dict[str, Any]

class SupabaseVectorService:
    """????????? ?????? ??????????? ?????? ????? Supabase"""
    
    def __init__(self, connection_string: str = None, table_name: str = "documents"):
        """
        ????????????? ???????
        
        Args:
            connection_string: ?????? ??????????? ? Supabase
            table_name: ??? ??????? ? ????????? (?? ????????? ?? LlamaIndex)
        """
        self.connection_string = connection_string or os.getenv("SUPABASE_CONNECTION_STRING")
        self.table_name = table_name
        self.embedding_model = None
        
        if not self.connection_string:
            raise ValueError("SUPABASE_CONNECTION_STRING not found!")
        
        # ????????????? embedding ??????
        self._init_embedding_model()
        
        logger.info(f"? Supabase Vector Service initialized (table: {self.table_name})")
    
    def _init_embedding_model(self):
        """????????????? embedding ?????? (?? ?? ??? ?????????????? ??? ??????????)"""
        try:
            from llama_index.embeddings.ollama import OllamaEmbedding
            
            # ????????: ?????????? nomic-embed-text ?????? mxbai-embed-large
            self.embedding_model = OllamaEmbedding(
                model_name="nomic-embed-text",
                base_url="http://localhost:11434"
            )
            logger.info("? Embedding model initialized (nomic-embed-text)")
            
        except ImportError:
            logger.error("? OllamaEmbedding not available - install llama-index-embeddings-ollama")
            self.embedding_model = None
        except Exception as e:
            logger.error(f"? Error initializing embedding model: {e}")
            self.embedding_model = None
    
    async def get_query_embedding(self, query: str) -> List[float]:
        """???????? embedding ??? ???????"""
        if not self.embedding_model:
            raise RuntimeError("Embedding model not initialized")
        
        try:
            # ???????? embedding ????? LlamaIndex
            embedding = self.embedding_model.get_text_embedding(query)
            return embedding
            
        except Exception as e:
            logger.error(f"Error getting query embedding: {e}")
            raise
    
    async def vector_search(self, 
                           query: str, 
                           limit: int = 10,
                           similarity_threshold: float = 0.3) -> List[SearchResult]:
        """
        ???????????? ????????? ????? - ????? ????? ??? ????????????
        
        Args:
            query: ????????? ?????? (??? ???? ??????????? ????????)
            limit: ?????????? ???????????
            similarity_threshold: ????? ???????? ??????
            
        Returns:
            ?????? ??????????? ??????
        """
        search_start = time.time()
        
        try:
            # ???????? embedding ??? ???????
            embedding_start = time.time()
            query_embedding = await self.get_query_embedding(query)
            embedding_time = time.time() - embedding_start
            
            logger.info(f"?? Query embedding generated in {embedding_time:.3f}s")
            
            # ????????? ????????? ????? ????? SQL
            search_sql_start = time.time()
            results = await self._execute_vector_search(
                query_embedding, 
                limit * 2,  # ????? ?????? ??? ?????? ??????????
                similarity_threshold
            )
            search_sql_time = time.time() - search_sql_start
            
            logger.info(f"?? Vector search completed in {search_sql_time:.3f}s (found {len(results)} candidates)")
            
            # ???????????? ? ??????????? ??????????
            processing_start = time.time()
            formatted_results = await self._process_search_results(results, query, limit)
            processing_time = time.time() - processing_start
            
            total_time = time.time() - search_start
            
            logger.info(f"? Search completed: {len(formatted_results)} results in {total_time:.3f}s")
            logger.info(f"   Breakdown: embedding={embedding_time:.3f}s, search={search_sql_time:.3f}s, processing={processing_time:.3f}s")
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"? Vector search failed: {e}")
            return []
    
    async def _execute_vector_search(self, 
                                   query_embedding: List[float], 
                                   limit: int,
                                   threshold: float) -> List[Dict]:
        """????????? SQL ?????? ??? ?????????? ??????"""
        try:
            import psycopg2
            import psycopg2.extras
            
            # ??????????? ? ????
            conn = psycopg2.connect(self.connection_string)
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # SQL ?????? ??? ?????????? ?????? - ??????? ????????? 'vec'
            query_sql = f"""
            SELECT 
                id,
                metadata,
                (metadata->>'text') as text_content,
                (metadata->>'file_name') as file_name,
                (metadata->>'file_path') as file_path,
                (vec <=> %s::vector) as distance
            FROM vecs.{self.table_name}
            WHERE (vec <=> %s::vector) < %s
            ORDER BY distance ASC
            LIMIT %s
            """
            
            # ??????????? embedding ? ?????? ??? PostgreSQL
            embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
            
            # ????????? ??????
            cur.execute(query_sql, (embedding_str, embedding_str, 1.0 - threshold, limit))
            results = cur.fetchall()
            
            logger.info(f"SQL query returned {len(results)} results")
            
            cur.close()
            conn.close()
            
            # ??????????? RealDict ? ??????? ???????
            return [dict(row) for row in results]
            
        except Exception as e:
            logger.error(f"? SQL vector search failed: {e}")
            return []
    
    async def _process_search_results(self, 
                                    raw_results: List[Dict], 
                                    query: str, 
                                    limit: int) -> List[SearchResult]:
        """???????????? ????? ?????????? ?????? ? ??????? ?????? ???????"""
        processed_results = []
        query_lower = query.lower()
        seen_documents = set()  # ??? ????????????
        
        for result in raw_results:
            try:
                # ????????? ??????
                distance = float(result.get('distance', 1.0))
                text_content = result.get('text_content', '')
                metadata = result.get('metadata', {})
                
                if not text_content or len(text_content.strip()) < 10:
                    continue
                
                # ??????????? similarity score (???????? ???????? ?? ????????? ????????)
                similarity_score = self._normalize_similarity_score(distance)
                
                # ????????? ??????????????
                file_name = result.get('file_name') or metadata.get('file_name', 'Unknown')
                file_path = result.get('file_path') or metadata.get('file_path', '')
                document_id = result.get('id', '')
                
                # ???????????? ?? ?????? (??? ? ?????? ??????)
                doc_key = f"{file_name}_{hash(text_content[:100])}"
                if doc_key in seen_documents:
                    logger.debug(f"Skipping duplicate document: {file_name}")
                    continue
                seen_documents.add(doc_key)
                
                # ?????????? ??? ??????????
                exact_match = query_lower in text_content.lower()
                semantic_match = similarity_score > 0.7
                
                # ??????? ?????? ???????? (???????? ??????????? ?????????? ????????)
                best_context = self._find_best_context(text_content, query, max_length=400)
                
                # ????????? chunk_index ?? ??????????????
                chunk_index = metadata.get('chunk_index', 0)
                if chunk_index is None:
                    chunk_index = 0
                
                # ??????? ?????????
                search_result = SearchResult(
                    content=best_context,
                    full_content=text_content,
                    metadata=metadata,
                    similarity_score=similarity_score,
                    document_id=document_id,
                    filename=file_name,
                    chunk_index=chunk_index,
                    search_info={
                        "query": query,
                        "distance": distance,
                        "match_type": "exact" if exact_match else ("semantic" if semantic_match else "weak"),
                        "confidence": "high" if similarity_score > 0.7 else ("medium" if similarity_score > 0.5 else "low"),
                        "search_method": "classic_vector"
                    }
                )
                
                processed_results.append(search_result)
                
            except Exception as e:
                logger.warning(f"Error processing search result: {e}")
                continue
        
        # ?????????: ??????? ?????? ??????????, ????? ?? similarity score
        processed_results.sort(key=lambda x: (
            x.search_info["match_type"] == "exact",
            x.similarity_score
        ), reverse=True)
        
        return processed_results[:limit]
    
    def _normalize_similarity_score(self, distance: float) -> float:
        """
        ??????????? distance ? similarity score (?? ?????? ??????)
        
        Args:
            distance: ?????????? ?? ?????????? ?????? (?????? = ?????)
            
        Returns:
            Similarity score ?? 0 ?? 1 (?????? = ?????)
        """
        # ???????????? ???? ?? ?????? ??????
        if distance <= 0:
            return 1.0  # ????????? ??????????
        elif distance >= 2.0:
            return 0.0  # ????? ?????? ??????????
        else:
            # ??????????? distance ?? 0-2 ? similarity_score ?? 1-0
            return max(0.0, (2.0 - distance) / 2.0)
    
    def _find_best_context(self, content: str, query: str, max_length: int = 400) -> str:
        """
        ??????? ????????? ??????????? ???? ????????? (?? ?????? ??????)
        
        Args:
            content: ?????? ????? ?????????
            query: ????????? ??????
            max_length: ???????????? ????? ?????????
            
        Returns:
            ?????? ???????? ??????
        """
        if len(content) <= max_length:
            return content
        
        query_words = query.lower().split()
        content_lower = content.lower()
        
        # ???? ?????? ???? ??? ?????? ?????????????
        best_score = 0
        best_start = 0
        
        # ????????? ????????? ???????? ? ??????
        for start in range(0, len(content) - max_length + 1, max_length // 4):
            end = start + max_length
            segment = content_lower[start:end]
            
            # ??????? ?????????? ???????? ???? ??????? ? ????????
            score = sum(1 for word in query_words if word in segment)
            
            # ????? ?? ?????????? ? ?????? ?????????
            if start == 0:
                score += 0.5
            
            if score > best_score:
                best_score = score
                best_start = start
        
        # ???? ?? ????? ??????? ???????, ?????????? ??????
        if best_score == 0:
            return content[:max_length] + "..."
        
        # ?????????? ?????? ???????
        best_end = best_start + max_length
        context = content[best_start:best_end]
        
        # ????????? ????????? ???? ????????
        if best_start > 0:
            context = "..." + context
        if best_end < len(content):
            context = context + "..."
        
        return context.strip()
    
    async def get_database_stats(self) -> Dict[str, Any]:
        """???????? ?????????? ???? ??????"""
        try:
            import psycopg2
            
            conn = psycopg2.connect(self.connection_string)
            cur = conn.cursor()
            
            # ???????? ????? ?????????? ??????????
            cur.execute(f"SELECT COUNT(*) FROM vecs.{self.table_name}")
            total_docs = cur.fetchone()[0]
            
            # ???????? ?????????? ??????
            cur.execute(f"""
                SELECT COUNT(DISTINCT metadata->>'file_name') 
                FROM vecs.{self.table_name} 
                WHERE metadata->>'file_name' IS NOT NULL
            """)
            unique_files = cur.fetchone()[0]
            
            cur.close()
            conn.close()
            
            return {
                "total_documents": total_docs,
                "unique_files": unique_files,
                "table_name": self.table_name,
                "search_method": "classic_vector",
                "embedding_model": "nomic-embed-text"  # ???????? ? mxbai-embed-large
            }
            
        except Exception as e:
            logger.error(f"Error getting database stats: {e}")
            return {
                "total_documents": 0,
                "unique_files": 0,
                "error": str(e)
            }