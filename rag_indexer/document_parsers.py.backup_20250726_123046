#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Document parsers module for RAG Document Indexer
Specialized parsers for different document types
NOW WITH ENHANCED PDF SUPPORT using hybrid PDF processor
"""

import os
import io
import zipfile
import tempfile
from pathlib import Path
from datetime import datetime
from llama_index.core import Document

# Advanced document parsing imports
try:
    from docx import Document as DocxDocument
    from docx.shared import Inches
    from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("INFO: python-docx not available. Install with: pip install python-docx")

try:
    import pypandoc
    PANDOC_AVAILABLE = True
except ImportError:
    PANDOC_AVAILABLE = False
    print("INFO: pypandoc not available. Install with: pip install pypandoc")

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# NEW: Enhanced PDF processing imports
try:
    import fitz  # PyMuPDF
    import pdfplumber
    from pdf2image import convert_from_path
    PDF_PROCESSING_AVAILABLE = True
except ImportError:
    PDF_PROCESSING_AVAILABLE = False
    print("INFO: PDF processing libraries not fully available. Install with: pip install PyMuPDF pdfplumber pdf2image")

# Import utility functions
from file_utils_core import clean_content_from_null_bytes, clean_metadata_recursive, safe_read_file


class EnhancedPDFProcessor:
    """
    NEW: Enhanced PDF processor using hybrid approach for optimal text extraction
    Integrated into the existing document parsing system
    """
    
    def __init__(self, config=None):
        """
        Initialize Enhanced PDF processor
        
        Args:
            config: Configuration object with PDF settings
        """
        self.config = config
        
        # Load settings from config or use defaults
        if config:
            self.chunk_size = getattr(config, 'PDF_CHUNK_SIZE', 2048)
            self.preserve_structure = getattr(config, 'PDF_PRESERVE_STRUCTURE', True)
            self.min_section_length = getattr(config, 'PDF_MIN_SECTION_LENGTH', 200)
            self.header_detection = getattr(config, 'PDF_HEADER_DETECTION', True)
            self.footer_cleanup = getattr(config, 'PDF_FOOTER_CLEANUP', True)
            self.enable_ocr_fallback = getattr(config, 'ENABLE_OCR', True)
        else:
            # Default settings
            self.chunk_size = 2048
            self.preserve_structure = True
            self.min_section_length = 200
            self.header_detection = True
            self.footer_cleanup = True
            self.enable_ocr_fallback = True
        
        # Check library availability
        self.libraries_available = {
            'pymupdf': self._check_pymupdf(),
            'pdfplumber': self._check_pdfplumber(), 
            'pdf2image': self._check_pdf2image()
        }
        
        # OCR processor (will be injected if needed)
        self.ocr_processor = None
        
        # Processing statistics
        self.stats = {
            'files_processed': 0,
            'total_pages': 0,
            'text_extracted_chars': 0,
            'ocr_pages': 0,
            'structured_pages': 0,
            'processing_time': 0,
            'method_usage': {
                'pymupdf_primary': 0,
                'pdfplumber_tables': 0,
                'ocr_fallback': 0,
                'failed_extractions': 0
            }
        }
    
    def _check_pymupdf(self):
        """Check if PyMuPDF is available"""
        try:
            import fitz
            return True
        except ImportError:
            return False
    
    def _check_pdfplumber(self):
        """Check if pdfplumber is available"""
        try:
            import pdfplumber
            return True
        except ImportError:
            return False
    
    def _check_pdf2image(self):
        """Check if pdf2image is available"""
        try:
            from pdf2image import convert_from_path
            return True
        except ImportError:
            return False
    
    def set_ocr_processor(self, ocr_processor):
        """
        Set OCR processor for fallback processing
        
        Args:
            ocr_processor: OCR processor instance
        """
        self.ocr_processor = ocr_processor
    
    def detect_pdf_type(self, file_path):
        """
        Detect PDF type to choose optimal processing strategy
        
        Args:
            file_path: Path to PDF file
        
        Returns:
            dict: PDF analysis results
        """
        analysis = {
            'type': 'unknown',
            'has_text': False,
            'has_images': False,
            'has_tables': False,
            'is_scanned': False,
            'page_count': 0,
            'text_coverage': 0.0,
            'recommended_method': 'pymupdf'
        }
        
        if not self.libraries_available['pymupdf']:
            analysis['recommended_method'] = 'fallback'
            return analysis
        
        try:
            import fitz
            # Quick analysis with PyMuPDF
            doc = fitz.open(file_path)
            analysis['page_count'] = len(doc)
            
            # Sample first few pages for analysis
            sample_pages = min(3, len(doc))
            total_text_length = 0
            total_char_count = 0
            
            for page_num in range(sample_pages):
                page = doc[page_num]
                
                # Extract text to check coverage
                text = page.get_text()
                total_text_length += len(text.strip())
                
                # Count characters for density calculation
                char_count = len([c for c in text if c.isalnum()])
                total_char_count += char_count
                
                # Check for images
                image_list = page.get_images()
                if image_list:
                    analysis['has_images'] = True
                
                # Basic table detection (look for table-like structures)
                if self._detect_table_patterns(text):
                    analysis['has_tables'] = True
            
            doc.close()
            
            # Determine PDF characteristics
            analysis['has_text'] = total_text_length > 50
            analysis['text_coverage'] = total_char_count / (sample_pages * 1000) if sample_pages > 0 else 0
            
            # Classify PDF type
            if analysis['text_coverage'] < 0.1:
                analysis['type'] = 'scanned'
                analysis['is_scanned'] = True
                analysis['recommended_method'] = 'ocr'
            elif analysis['has_tables'] and self.libraries_available['pdfplumber']:
                analysis['type'] = 'structured'
                analysis['recommended_method'] = 'pdfplumber'
            elif analysis['has_text']:
                analysis['type'] = 'digital'
                analysis['recommended_method'] = 'pymupdf'
            else:
                analysis['type'] = 'mixed'
                analysis['recommended_method'] = 'hybrid'
            
        except Exception as e:
            print(f"   WARNING: PDF analysis failed: {e}")
            analysis['recommended_method'] = 'fallback'
        
        return analysis
    
    def _detect_table_patterns(self, text):
        """
        Simple heuristic to detect table-like content
        
        Args:
            text: Text content to analyze
        
        Returns:
            bool: True if table patterns detected
        """
        if not text:
            return False
        
        lines = text.split('\n')
        if len(lines) < 3:
            return False
        
        # Look for patterns indicating tables
        tab_separated_lines = sum(1 for line in lines if '\t' in line)
        space_separated_lines = sum(1 for line in lines if len(line.split()) > 4)
        
        # If more than 30% of lines look table-like
        table_ratio = (tab_separated_lines + space_separated_lines) / len(lines)
        return table_ratio > 0.3
    
    def extract_text_pymupdf(self, file_path):
        """
        Extract text using enhanced PyMuPDF extraction methods
        
        Args:
            file_path: Path to PDF file
        
        Returns:
            tuple: (text_content, extraction_info)
        """
        if not self.libraries_available['pymupdf']:
            return "", {'error': 'PyMuPDF not available'}
        
        try:
            import fitz
            import time
            
            doc = fitz.open(file_path)
            text_parts = []
            extraction_info = {
                'method': 'enhanced_pymupdf',
                'pages_processed': 0,
                'total_chars': 0,
                'processing_time': 0,
                'extraction_modes_used': []
            }
            
            start_time = time.time()
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                # Enhanced extraction: try multiple methods and use the best
                extraction_methods = []
                
                # Method 1: Standard text extraction
                text1 = page.get_text("text") if self.preserve_structure else page.get_text()
                extraction_methods.append(('standard', text1))
                
                # Method 2: Blocks extraction
                try:
                    blocks = page.get_text("blocks")
                    text2 = ""
                    if isinstance(blocks, list):
                        for block in blocks:
                            if isinstance(block, tuple) and len(block) > 4:
                                text2 += str(block[4]) + " "
                            elif isinstance(block, dict) and 'text' in block:
                                text2 += block['text'] + " "
                    extraction_methods.append(('blocks', text2))
                except:
                    pass
                
                # Method 3: Words extraction
                try:
                    words = page.get_text("words")
                    text3 = ""
                    if isinstance(words, list):
                        text3 = " ".join([str(word[4]) for word in words 
                                        if isinstance(word, tuple) and len(word) > 4])
                    extraction_methods.append(('words', text3))
                except:
                    pass
                
                # Method 4: Dictionary extraction
                try:
                    text_dict = page.get_text("dict")
                    text4 = self._extract_from_dict(text_dict)
                    extraction_methods.append(('dict', text4))
                except:
                    pass
                
                # Choose the best extraction method for this page
                best_text = ""
                best_method = "standard"
                max_chars = 0
                
                for method_name, text in extraction_methods:
                    if text and len(text.strip()) > max_chars:
                        best_text = text.strip()
                        best_method = method_name
                        max_chars = len(best_text)
                
                if best_text:
                    # Clean and process text
                    cleaned_text = clean_content_from_null_bytes(best_text)
                    
                    # Optional header/footer cleanup
                    if self.footer_cleanup:
                        cleaned_text = self._clean_headers_footers(cleaned_text, page_num)
                    
                    text_parts.append(cleaned_text)
                    extraction_info['total_chars'] += len(cleaned_text)
                    
                    # Track which method worked best
                    if best_method not in extraction_info['extraction_modes_used']:
                        extraction_info['extraction_modes_used'].append(best_method)
                
                extraction_info['pages_processed'] += 1
            
            doc.close()
            
            extraction_info['processing_time'] = time.time() - start_time
            
            # Combine text with proper spacing
            full_text = '\n\n'.join(text_parts)
            
            # Update statistics
            self.stats['method_usage']['pymupdf_primary'] += 1
            
            return full_text, extraction_info
            
        except Exception as e:
            return "", {'error': str(e), 'method': 'enhanced_pymupdf'}
    
    def _extract_from_dict(self, text_dict):
        """
        Extract text from PyMuPDF dictionary format (enhanced method)
        
        Args:
            text_dict: Dictionary from get_text("dict")
        
        Returns:
            str: Extracted text
        """
        text_parts = []
        
        try:
            if isinstance(text_dict, dict) and 'blocks' in text_dict:
                for block in text_dict['blocks']:
                    if isinstance(block, dict) and 'lines' in block:
                        for line in block['lines']:
                            if isinstance(line, dict) and 'spans' in line:
                                line_text = ""
                                for span in line['spans']:
                                    if isinstance(span, dict) and 'text' in span:
                                        line_text += span['text']
                                if line_text.strip():
                                    text_parts.append(line_text.strip())
        except Exception:
            pass
        
        return ' '.join(text_parts)
    
    def extract_text_pdfplumber(self, file_path):
        """
        Extract text using pdfplumber (best for tables and complex structures)
        
        Args:
            file_path: Path to PDF file
        
        Returns:
            tuple: (text_content, extraction_info)
        """
        if not self.libraries_available['pdfplumber']:
            return "", {'error': 'pdfplumber not available'}
        
        try:
            import pdfplumber
            import time
            
            text_parts = []
            extraction_info = {
                'method': 'pdfplumber',
                'pages_processed': 0,
                'tables_found': 0,
                'total_chars': 0,
                'processing_time': 0
            }
            
            start_time = time.time()
            
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = ""
                    
                    # Extract regular text
                    if self.preserve_structure:
                        text = page.extract_text(layout=True)
                    else:
                        text = page.extract_text()
                    
                    if text:
                        page_text += text
                    
                    # Extract tables separately for better formatting
                    try:
                        tables = page.extract_tables()
                        if tables:
                            extraction_info['tables_found'] += len(tables)
                            for table in tables:
                                table_text = self._format_table_text(table)
                                if table_text:
                                    page_text += f"\n\n[TABLE]\n{table_text}\n[/TABLE]\n"
                    except Exception as e:
                        print(f"   WARNING: Table extraction failed on page {page_num + 1}: {e}")
                    
                    if page_text.strip():
                        cleaned_text = clean_content_from_null_bytes(page_text)
                        text_parts.append(cleaned_text)
                        extraction_info['total_chars'] += len(cleaned_text)
                    
                    extraction_info['pages_processed'] += 1
            
            extraction_info['processing_time'] = time.time() - start_time
            
            # Combine text
            full_text = '\n\n'.join(text_parts)
            
            # Update statistics
            self.stats['method_usage']['pdfplumber_tables'] += 1
            
            return full_text, extraction_info
            
        except Exception as e:
            return "", {'error': str(e), 'method': 'pdfplumber'}
    
    def _format_table_text(self, table):
        """
        Format extracted table data into readable text
        
        Args:
            table: Table data from pdfplumber
        
        Returns:
            str: Formatted table text
        """
        if not table:
            return ""
        
        try:
            formatted_rows = []
            for row in table:
                if row:
                    # Clean and join cells
                    cells = [str(cell).strip() if cell else "" for cell in row]
                    row_text = " | ".join(cells)
                    if row_text.strip():
                        formatted_rows.append(row_text)
            
            return '\n'.join(formatted_rows)
        except Exception as e:
            print(f"   WARNING: Table formatting failed: {e}")
            return ""
    
    def _clean_headers_footers(self, text, page_num):
        """
        Basic header/footer cleanup
        
        Args:
            text: Text content
            page_num: Page number
        
        Returns:
            str: Cleaned text
        """
        if not text:
            return text
        
        lines = text.split('\n')
        if len(lines) < 5:
            return text
        
        # Remove common header/footer patterns
        cleaned_lines = []
        for i, line in enumerate(lines):
            line = line.strip()
            
            # Skip very short lines at top/bottom
            if (i < 2 or i >= len(lines) - 2) and len(line) < 50:
                # Check if it's just page numbers or headers
                if line.isdigit() or 'page' in line.lower():
                    continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def extract_text_ocr_fallback(self, file_path):
        """
        Extract text using OCR fallback (for scanned PDFs)
        
        Args:
            file_path: Path to PDF file
        
        Returns:
            tuple: (text_content, extraction_info)
        """
        if not (self.libraries_available['pdf2image'] and self.ocr_processor):
            return "", {'error': 'OCR fallback not available'}
        
        try:
            from pdf2image import convert_from_path
            import time
            
            extraction_info = {
                'method': 'ocr_fallback',
                'pages_processed': 0,
                'total_chars': 0,
                'processing_time': 0
            }
            
            start_time = time.time()
            
            # Convert PDF pages to images
            images = convert_from_path(file_path, dpi=300, fmt='jpeg')
            text_parts = []
            
            for page_num, image in enumerate(images):
                try:
                    # Save image temporarily for OCR processing
                    temp_image_path = f"/tmp/pdf_page_{page_num}.jpg"
                    image.save(temp_image_path, 'JPEG')
                    
                    # Process with OCR
                    text = self.ocr_processor.extract_text_from_image(temp_image_path)
                    
                    if text and len(text.strip()) > 20:
                        cleaned_text = clean_content_from_null_bytes(text)
                        text_parts.append(cleaned_text)
                        extraction_info['total_chars'] += len(cleaned_text)
                    
                    # Clean up temporary file
                    if os.path.exists(temp_image_path):
                        os.unlink(temp_image_path)
                    
                    extraction_info['pages_processed'] += 1
                    
                except Exception as e:
                    print(f"   WARNING: OCR failed for page {page_num + 1}: {e}")
            
            extraction_info['processing_time'] = time.time() - start_time
            
            # Combine text
            full_text = '\n\n'.join(text_parts)
            
            # Update statistics
            self.stats['method_usage']['ocr_fallback'] += 1
            
            return full_text, extraction_info
            
        except Exception as e:
            return "", {'error': str(e), 'method': 'ocr_fallback'}
    
    def process_pdf_file(self, file_path):
        """
        Process a single PDF file using optimal strategy
        
        Args:
            file_path: Path to PDF file
        
        Returns:
            list: List of Document objects
        """
        print(f"   Ì†ΩÌ≥Ñ Processing PDF: {os.path.basename(file_path)}")
        
        # Analyze PDF to choose strategy
        pdf_analysis = self.detect_pdf_type(file_path)
        print(f"   Ì†ΩÌ≥ä PDF type: {pdf_analysis['type']} ({pdf_analysis['page_count']} pages)")
        print(f"   Ì†ºÌæØ Strategy: {pdf_analysis['recommended_method']}")
        
        # Extract text using optimal method
        extraction_method = pdf_analysis['recommended_method']
        text_content = ""
        extraction_info = {}
        
        if extraction_method == 'pymupdf' and self.libraries_available['pymupdf']:
            text_content, extraction_info = self.extract_text_pymupdf(file_path)
        elif extraction_method == 'pdfplumber' and self.libraries_available['pdfplumber']:
            text_content, extraction_info = self.extract_text_pdfplumber(file_path)
        elif extraction_method == 'ocr' and self.ocr_processor:
            text_content, extraction_info = self.extract_text_ocr_fallback(file_path)
        else:
            # Fallback chain
            if self.libraries_available['pymupdf']:
                text_content, extraction_info = self.extract_text_pymupdf(file_path)
            elif self.libraries_available['pdfplumber']:
                text_content, extraction_info = self.extract_text_pdfplumber(file_path)
            else:
                print(f"   ‚ùå No PDF processing libraries available")
                self.stats['method_usage']['failed_extractions'] += 1
                return []
        
        # Check extraction result
        if 'error' in extraction_info:
            print(f"   ‚ùå Extraction failed: {extraction_info['error']}")
            self.stats['method_usage']['failed_extractions'] += 1
            return []
        
        if not text_content or len(text_content.strip()) < 20:
            print(f"   ‚ö†Ô∏è Insufficient text extracted ({len(text_content)} chars)")
            if self.enable_ocr_fallback and self.ocr_processor and extraction_method != 'ocr':
                print(f"   Ì†ΩÌ¥Ñ Trying OCR fallback...")
                text_content, ocr_info = self.extract_text_ocr_fallback(file_path)
                if text_content and len(text_content.strip()) >= 20:
                    extraction_info = ocr_info
                else:
                    self.stats['method_usage']['failed_extractions'] += 1
                    return []
            else:
                self.stats['method_usage']['failed_extractions'] += 1
                return []
        
        # Create document with enhanced metadata
        clean_file_path = clean_content_from_null_bytes(str(file_path))
        clean_file_name = clean_content_from_null_bytes(os.path.basename(file_path))
        
        metadata = {
            'file_path': clean_file_path,
            'file_name': clean_file_name,
            'file_type': 'pdf',
            'file_size': os.path.getsize(file_path),
            'pdf_analysis': pdf_analysis,
            'extraction_info': extraction_info,
            'content_length': len(text_content),
            'processing_timestamp': datetime.now().isoformat(),
            'processor_version': 'enhanced_pdf_processor_v1.0'
        }
        
        # Clean metadata
        clean_metadata = clean_metadata_recursive(metadata)
        
        document = Document(
            text=text_content,
            metadata=clean_metadata
        )
        
        # Update statistics
        self.stats['files_processed'] += 1
        self.stats['total_pages'] += pdf_analysis['page_count']
        self.stats['text_extracted_chars'] += len(text_content)
        
        if extraction_info['method'] == 'ocr_fallback':
            self.stats['ocr_pages'] += extraction_info.get('pages_processed', 0)
        else:
            self.stats['structured_pages'] += extraction_info.get('pages_processed', 0)
        
        print(f"   ‚úÖ SUCCESS: {len(text_content)} characters extracted in {extraction_info.get('processing_time', 0):.2f}s")
        
        return [document]


class AdvancedDocxParser:
    """Advanced parser for .docx files with image extraction and structure preservation"""
    
    def __init__(self, extract_images=True, preserve_structure=True, extract_tables=True):
        """
        Initialize advanced DOCX parser
        
        Args:
            extract_images: Whether to extract images from documents
            preserve_structure: Whether to preserve document structure (headers, etc.)
            extract_tables: Whether to extract table content
        """
        self.extract_images = extract_images
        self.preserve_structure = preserve_structure
        self.extract_tables = extract_tables
        self.is_available = DOCX_AVAILABLE and PIL_AVAILABLE
        
        if not self.is_available:
            print("WARNING: Advanced DOCX parsing not available. Missing dependencies.")
    
    def extract_images_from_docx(self, docx_file_path):
        """
        Extract all images from DOCX file
        
        Args:
            docx_file_path: Path to DOCX file
        
        Returns:
            list: List of (image_data, image_name, image_format) tuples
        """
        if not self.is_available or not self.extract_images:
            return []
        
        images = []
        
        try:
            # Open DOCX as ZIP file to access images
            with zipfile.ZipFile(docx_file_path, 'r') as docx_zip:
                # Find all image files in the ZIP
                image_files = [f for f in docx_zip.namelist() 
                              if f.startswith('word/media/') and 
                              any(f.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.bmp'])]
                
                for i, image_file in enumerate(image_files):
                    try:
                        # Extract image data
                        image_data = docx_zip.read(image_file)
                        
                        # Determine image format
                        image_name = os.path.basename(image_file)
                        image_format = os.path.splitext(image_name)[1].lower()
                        
                        # Create a more descriptive name
                        base_name = os.path.splitext(os.path.basename(docx_file_path))[0]
                        enhanced_name = f"{base_name}_image_{i+1}{image_format}"
                        
                        images.append((image_data, enhanced_name, image_format))
                        
                    except Exception as e:
                        print(f"   WARNING: Failed to extract image {image_file}: {e}")
                
            if images:
                print(f"   INFO: Extracted {len(images)} images from {os.path.basename(docx_file_path)}")
                
        except Exception as e:
            print(f"   ERROR: Failed to extract images from {docx_file_path}: {e}")
        
        return images
    
    def extract_table_content(self, doc):
        """
        Extract content from all tables in the document
        
        Args:
            doc: python-docx Document object
        
        Returns:
            str: Formatted table content
        """
        if not self.extract_tables:
            return ""
        
        table_content = []
        
        try:
            for i, table in enumerate(doc.tables):
                table_text = f"\n--- Table {i+1} ---\n"
                
                for row in table.rows:
                    row_cells = []
                    for cell in row.cells:
                        # Clean cell text and remove extra whitespace
                        cell_text = ' '.join(cell.text.strip().split())
                        row_cells.append(cell_text)
                    
                    # Join cells with tab separator
                    table_text += '\t'.join(row_cells) + '\n'
                
                table_text += "--- End Table ---\n"
                table_content.append(table_text)
                
        except Exception as e:
            print(f"   WARNING: Failed to extract table content: {e}")
        
        return '\n'.join(table_content)
    
    def extract_structured_content(self, doc):
        """
        Extract content while preserving document structure
        
        Args:
            doc: python-docx Document object
        
        Returns:
            tuple: (structured_text, metadata_info)
        """
        if not self.preserve_structure:
            # Simple text extraction
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            return '\n'.join(full_text), {}
        
        structured_content = []
        metadata_info = {
            'headings': [],
            'paragraph_count': 0,
            'heading_count': 0,
            'list_count': 0
        }
        
        try:
            for paragraph in doc.paragraphs:
                if not paragraph.text.strip():
                    continue
                
                # Check if paragraph is a heading
                if paragraph.style.name.startswith('Heading'):
                    level = paragraph.style.name.replace('Heading ', '')
                    heading_marker = '#' * min(int(level) if level.isdigit() else 1, 6)
                    structured_content.append(f"{heading_marker} {paragraph.text.strip()}")
                    
                    metadata_info['headings'].append({
                        'level': level,
                        'text': paragraph.text.strip()
                    })
                    metadata_info['heading_count'] += 1
                
                # Check if paragraph is a list item
                elif paragraph.style.name.startswith('List'):
                    structured_content.append(f"‚Ä¢ {paragraph.text.strip()}")
                    metadata_info['list_count'] += 1
                
                # Regular paragraph
                else:
                    structured_content.append(paragraph.text.strip())
                    metadata_info['paragraph_count'] += 1
            
        except Exception as e:
            print(f"   WARNING: Failed to extract structured content: {e}")
            # Fallback to simple extraction
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            return '\n'.join(full_text), {'extraction_fallback': True}
        
        return '\n'.join(structured_content), metadata_info
    
    def parse_docx_file(self, file_path):
        """
        Parse DOCX file with advanced features
        
        Args:
            file_path: Path to DOCX file
        
        Returns:
            tuple: (main_document, extracted_images, parsing_info)
        """
        if not self.is_available:
            print(f"   WARNING: Advanced DOCX parsing not available for {os.path.basename(file_path)}")
            return None, [], {'error': 'dependencies_missing'}
        
        try:
            print(f"   INFO: Advanced parsing of {os.path.basename(file_path)}")
            
            # Open document
            doc = DocxDocument(file_path)
            
            # Extract structured text content
            structured_text, structure_metadata = self.extract_structured_content(doc)
            
            # Extract table content
            table_content = self.extract_table_content(doc)
            
            # Combine text and table content
            full_text = structured_text
            if table_content:
                full_text += f"\n\n{table_content}"
            
            # Clean text
            full_text = clean_content_from_null_bytes(full_text)
            
            # Extract images
            extracted_images = self.extract_images_from_docx(file_path)
            
            # Create enhanced metadata
            parsing_info = {
                'parser_type': 'advanced_docx',
                'structure_preserved': self.preserve_structure,
                'images_extracted': len(extracted_images),
                'tables_found': len(doc.tables) if hasattr(doc, 'tables') else 0,
                'structure_metadata': structure_metadata,
                'extraction_features': {
                    'extract_images': self.extract_images,
                    'preserve_structure': self.preserve_structure,
                    'extract_tables': self.extract_tables
                }
            }
            
            # Create main document
            clean_file_path = clean_content_from_null_bytes(str(file_path))
            clean_file_name = clean_content_from_null_bytes(os.path.basename(file_path))
            
            main_metadata = {
                'file_path': clean_file_path,
                'file_name': clean_file_name,
                'file_type': 'docx',
                'file_size': os.path.getsize(file_path),
                'parsing_info': parsing_info,
                'content_length': len(full_text),
                'extraction_method': 'advanced_docx_parser'
            }
            
            # Clean metadata
            clean_main_metadata = clean_metadata_recursive(main_metadata)
            
            main_document = Document(
                text=full_text,
                metadata=clean_main_metadata
            )
            
            print(f"   SUCCESS: Extracted {len(full_text)} characters, {len(extracted_images)} images")
            
            return main_document, extracted_images, parsing_info
            
        except Exception as e:
            print(f"   ERROR: Failed to parse DOCX file {file_path}: {e}")
            return None, [], {'error': str(e), 'parser_type': 'advanced_docx'}


class LegacyDocConverter:
    """Converter for legacy .doc files using pandoc or other methods"""
    
    def __init__(self):
        """Initialize legacy DOC converter"""
        self.pandoc_available = PANDOC_AVAILABLE
        
        if not self.pandoc_available:
            print("INFO: pandoc not available for .doc conversion. .doc files will use fallback methods.")
    
    def convert_doc_to_text(self, doc_file_path):
        """
        Convert .doc file to text using available methods
        
        Args:
            doc_file_path: Path to .doc file
        
        Returns:
            tuple: (text_content, conversion_info)
        """
        conversion_info = {'method': 'none', 'success': False}
        
        try:
            # Method 1: Try pandoc if available
            if self.pandoc_available:
                try:
                    text_content = pypandoc.convert_file(doc_file_path, 'plain')
                    text_content = clean_content_from_null_bytes(text_content)
                    
                    if text_content and text_content.strip():
                        conversion_info = {
                            'method': 'pandoc',
                            'success': True,
                            'content_length': len(text_content)
                        }
                        print(f"   INFO: Converted .doc using pandoc: {len(text_content)} characters")
                        return text_content, conversion_info
                
                except Exception as e:
                    print(f"   WARNING: Pandoc conversion failed: {e}")
            
            # Method 2: Try to read as binary and extract readable text (very basic)
            try:
                with open(doc_file_path, 'rb') as f:
                    raw_content = f.read()
                
                # Very basic text extraction from binary data
                # This is a fallback method and may not work well
                text_content = raw_content.decode('latin-1', errors='ignore')
                
                # Filter out non-printable characters and keep only readable text
                readable_chars = []
                for char in text_content:
                    if char.isprintable() or char.isspace():
                        readable_chars.append(char)
                
                filtered_content = ''.join(readable_chars)
                
                # Remove excessive whitespace and clean up
                lines = [line.strip() for line in filtered_content.split('\n') if line.strip()]
                final_content = '\n'.join(lines)
                
                # Clean null bytes
                final_content = clean_content_from_null_bytes(final_content)
                
                if final_content and len(final_content) > 100:  # Minimum reasonable length
                    conversion_info = {
                        'method': 'binary_extraction',
                        'success': True,
                        'content_length': len(final_content),
                        'warning': 'Low quality extraction method'
                    }
                    print(f"   WARNING: Using basic binary extraction for .doc file: {len(final_content)} characters")
                    return final_content, conversion_info
                
            except Exception as e:
                print(f"   WARNING: Binary extraction failed: {e}")
            
            # If all methods fail
            conversion_info = {
                'method': 'failed',
                'success': False,
                'error': 'No conversion method succeeded'
            }
            
            return "", conversion_info
            
        except Exception as e:
            conversion_info = {
                'method': 'error',
                'success': False,
                'error': str(e)
            }
            return "", conversion_info


class HybridDocumentProcessor:
    """Processor that combines text extraction with image OCR for complete document processing"""
    
    def __init__(self, config=None):
        """
        Initialize hybrid document processor
        
        Args:
            config: Configuration object with processing settings
        """
        self.config = config
        
        # Load settings from config
        if config:
            doc_settings = config.get_document_parsing_settings()
            self.advanced_parsing_enabled = doc_settings.get('advanced_parsing_enabled', True)
            self.extract_images = doc_settings.get('extract_images', True)
            self.preserve_structure = doc_settings.get('preserve_structure', True)
            self.extract_tables = doc_settings.get('extract_tables', True)
            self.hybrid_processing = doc_settings.get('hybrid_processing', True)
            self.combine_results = doc_settings.get('combine_results', True)
            self.image_quality = doc_settings.get('image_quality', 'high')
        else:
            # Default settings
            self.advanced_parsing_enabled = True
            self.extract_images = True
            self.preserve_structure = True
            self.extract_tables = True
            self.hybrid_processing = True
            self.combine_results = True
            self.image_quality = 'high'
        
        # Initialize specialized parsers
        self.docx_parser = AdvancedDocxParser(
            extract_images=self.extract_images,
            preserve_structure=self.preserve_structure,
            extract_tables=self.extract_tables
        ) if self.advanced_parsing_enabled else None
        
        self.doc_converter = LegacyDocConverter() if self.advanced_parsing_enabled else None
        
        # NEW: Initialize enhanced PDF processor
        self.pdf_processor = EnhancedPDFProcessor(config) if self.advanced_parsing_enabled else None
        
        # OCR processor will be injected when needed
        self.ocr_processor = None
    
    def set_ocr_processor(self, ocr_processor):
        """
        Set OCR processor for image processing
        
        Args:
            ocr_processor: OCR processor instance
        """
        self.ocr_processor = ocr_processor
        
        # Also set for PDF processor
        if self.pdf_processor:
            self.pdf_processor.set_ocr_processor(ocr_processor)
    
    def process_pdf_file(self, file_path):
        """
        NEW: Process PDF file with enhanced PDF processor
        
        Args:
            file_path: Path to PDF file
        
        Returns:
            list: List of Document objects
        """
        if not self.pdf_processor or not self.advanced_parsing_enabled:
            # Fallback to simple processing
            return self._simple_file_processing(file_path)
        
        try:
            # Use enhanced PDF processor
            documents = self.pdf_processor.process_pdf_file(file_path)
            return documents if documents else []
            
        except Exception as e:
            print(f"   ERROR: Enhanced PDF processing failed for {file_path}: {e}")
            return self._simple_file_processing(file_path)
    
    def process_docx_file(self, file_path):
        """
        Process DOCX file with advanced parsing and image extraction
        
        Args:
            file_path: Path to DOCX file
        
        Returns:
            list: List of Document objects (main document + image documents)
        """
        if not self.docx_parser or not self.advanced_parsing_enabled:
            # Fallback to simple processing
            return self._simple_file_processing(file_path)
        
        try:
            # Parse DOCX with advanced features
            main_document, extracted_images, parsing_info = self.docx_parser.parse_docx_file(file_path)
            
            documents = []
            
            # Add main document if successful
            if main_document:
                documents.append(main_document)
            
            # Process extracted images with OCR if enabled
            if extracted_images and self.hybrid_processing and self.ocr_processor:
                print(f"   INFO: Processing {len(extracted_images)} extracted images with OCR")
                
                for image_data, image_name, image_format in extracted_images:
                    try:
                        # Create temporary file for OCR processing
                        with tempfile.NamedTemporaryFile(suffix=image_format, delete=False) as temp_file:
                            temp_file.write(image_data)
                            temp_file_path = temp_file.name
                        
                        # Process with OCR
                        ocr_document = self.ocr_processor.process_single_image(temp_file_path)
                        
                        if ocr_document:
                            # Enhance metadata to indicate it's from a document
                            ocr_document.metadata.update({
                                'source_document': os.path.basename(file_path),
                                'extraction_method': 'docx_image_ocr',
                                'original_image_name': image_name
                            })
                            documents.append(ocr_document)
                        
                        # Clean up temporary file
                        os.unlink(temp_file_path)
                        
                    except Exception as e:
                        print(f"   WARNING: Failed to process extracted image {image_name}: {e}")
            
            return documents
            
        except Exception as e:
            print(f"   ERROR: Advanced DOCX processing failed for {file_path}: {e}")
            return self._simple_file_processing(file_path)
    
    def process_doc_file(self, file_path):
        """
        Process legacy .doc file
        
        Args:
            file_path: Path to .doc file
        
        Returns:
            list: List of Document objects
        """
        if not self.doc_converter or not self.advanced_parsing_enabled:
            return self._simple_file_processing(file_path)
        
        try:
            # Convert .doc to text
            text_content, conversion_info = self.doc_converter.convert_doc_to_text(file_path)
            
            if conversion_info['success'] and text_content:
                # Create document
                clean_file_path = clean_content_from_null_bytes(str(file_path))
                clean_file_name = clean_content_from_null_bytes(os.path.basename(file_path))
                
                metadata = {
                    'file_path': clean_file_path,
                    'file_name': clean_file_name,
                    'file_type': 'doc',
                    'file_size': os.path.getsize(file_path),
                    'conversion_info': conversion_info,
                    'content_length': len(text_content),
                    'extraction_method': 'legacy_doc_converter'
                }
                
                # Clean metadata
                clean_metadata = clean_metadata_recursive(metadata)
                
                document = Document(
                    text=text_content,
                    metadata=clean_metadata
                )
                
                return [document]
            else:
                print(f"   WARNING: Failed to convert .doc file: {file_path}")
                return self._simple_file_processing(file_path)
                
        except Exception as e:
            print(f"   ERROR: Legacy .doc processing failed for {file_path}: {e}")
            return self._simple_file_processing(file_path)
    
    def _simple_file_processing(self, file_path):
        """
        Fallback to simple file processing
        
        Args:
            file_path: Path to file
        
        Returns:
            list: List of Document objects
        """
        try:
            content, error_code = safe_read_file(file_path)
            
            if content:
                clean_file_path = clean_content_from_null_bytes(str(file_path))
                clean_file_name = clean_content_from_null_bytes(os.path.basename(file_path))
                
                metadata = {
                    'file_path': clean_file_path,
                    'file_name': clean_file_name,
                    'file_type': os.path.splitext(file_path)[1].lower(),
                    'file_size': os.path.getsize(file_path),
                    'extraction_method': 'simple_text_reader',
                    'content_length': len(content)
                }
                
                if error_code:
                    metadata['encoding_fallback'] = error_code
                
                # Clean metadata
                clean_metadata = clean_metadata_recursive(metadata)
                
                document = Document(
                    text=content,
                    metadata=clean_metadata
                )
                
                return [document]
            else:
                return []
                
        except Exception as e:
            print(f"   ERROR: Simple file processing failed for {file_path}: {e}")
            return []